{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________\n",
    "# Introduction to Pytorch - part2\n",
    "### (based on the [60 min blitz Deep Learning with Pytorch] (https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) )\n",
    "\n",
    "## 1. Tensors\n",
    "## 2. A Gentle Introduction to ``torch.autograd``\n",
    "## 3. Neural Networks\n",
    "## 4. Train a Classifier\n",
    "\n",
    "__________________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. A Gentle Introduction to ``torch.autograd``\n",
    "---------------------------------\n",
    "\n",
    "**``torch.autograd`` is PyTorchâ€™s automatic differentiation engine that powers\n",
    "neural network training. In this section, you will get a conceptual\n",
    "understanding of how autograd helps a neural network train.**\n",
    "\n",
    "**When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.**\n",
    "\n",
    "\r\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine calle``d torch.autogr``a\n",
    ". It supports automatic computation of gradient for any computational graph.\r\n",
    "\r\n",
    "Consider the simplest one-layer neural network, with input x, parameters w and b, and some loss function. It can be defined in PyTorch in the follo manner:ner:\n"
   ]
  },
  {
   "attachments": {
    "fe1accd1-01f9-4c8a-9a21-59d3acde2c8d.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![comp-graph.png](attachment:fe1accd1-01f9-4c8a-9a21-59d3acde2c8d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this network, w and b are parameters, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables. In order to do that, we set the ``requires_grad`` property of those tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** You can set the value of requires_grad when creating a tensor, or later by using ``x.requires_grad_(True)`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([[-0.1067, -0.2149, -1.2540],\n",
      "        [ 1.1494, -0.1962, -1.1731],\n",
      "        [ 0.3784, -0.4695, -1.9333],\n",
      "        [-1.0746,  0.4463,  0.3427],\n",
      "        [ 1.3316,  0.3336, -1.0046]], requires_grad=True)\n",
      "tensor([ 1.3599,  0.3068, -1.6394], requires_grad=True)\n",
      "tensor([ 3.0381,  0.2060, -6.6617], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0.])\n",
      "tensor(1.2959, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "print(x)\n",
    "print(w)\n",
    "print(b)\n",
    "print(z)\n",
    "\n",
    "print(y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that we apply to tensors to construct computational graph is in fact an object of class ``Function``. \n",
    "\n",
    "This object knows how to compute the function in the forward direction, and also how to compute its derivative during the backward propagation step. A reference to the backward propagation function is stored in grad_fn property of a tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for x = None\n",
      "Gradient function for y = None\n",
      "Gradient function for z = <AddBackward0 object at 0x000001E96C9FE260>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000001E96CBB8430>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for x = {x.grad_fn}\")\n",
    "print(f\"Gradient function for y = {y.grad_fn}\")\n",
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients\n",
    "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely, we need $$\\frac{dloss}{dw}$$ and $$\\frac{dloss}{db}$$ under some fixed values of ``x`` and ``y``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute those derivatives, we call ``loss.backward()``,and then retrieve the values from ``w.grad`` and ``b.grad``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad:\n",
      "tensor([[0.3181, 0.1838, 0.0004],\n",
      "        [0.3181, 0.1838, 0.0004],\n",
      "        [0.3181, 0.1838, 0.0004],\n",
      "        [0.3181, 0.1838, 0.0004],\n",
      "        [0.3181, 0.1838, 0.0004]])\n",
      "b.grad:\n",
      "tensor([0.3181, 0.1838, 0.0004])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ..\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(\"w.grad:\")\n",
    "print(w.grad)\n",
    "print(\"b.grad:\")\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "We can only obtain the grad properties for the leaf nodes of the computational graph, which have ``requires_grad`` property set to ``True``. \n",
    "\n",
    "For all other nodes in our graph, gradients will not be available.\n",
    "\n",
    "We can only perform gradient calculations using ``backward`` once on a given graph, for performance reasons.\n",
    "\n",
    "If we need to do several backward calls on the same graph, we need to pass ``retain_graph=True`` to the ``backward`` call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disabling Gradient Tracking\n",
    "By default, all tensors with ``requires_grad=True`` are tracking their computational history and support gradient computation.\n",
    "\n",
    "However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. \n",
    "\n",
    "We can stop tracking computations by surrounding our computation code with ``torch.no_grad()`` block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nwith torch.no_grad():\\n    z = torch.matmul(x, w)+b\\nprint(z.requires_grad)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## NN Background\n",
    "\n",
    "**Neural networks (NNs) are a collection of nested functions that are\n",
    "executed on some input data. These functions are defined by *parameters*\n",
    "(consisting of weights and biases), which in PyTorch are stored in\n",
    "tensors.**\n",
    "\n",
    "**Training a NN happens in two steps:**\n",
    "\n",
    "**Forward Propagation**: In forward prop, the NN makes its best guess\n",
    "about the correct output. It runs the input data through each of its\n",
    "functions to make this guess.\n",
    "\n",
    "**Backward Propagation**: In backprop, the NN adjusts its parameters\n",
    "proportionate to the error in its guess. It does this by traversing\n",
    "backwards from the output, collecting the derivatives of the error with\n",
    "respect to the parameters of the functions (*gradients*), and optimizing\n",
    "the parameters using gradient descent. For a more detailed walkthrough\n",
    "of backprop, check out this [video from\n",
    "3Blue1Brown] (https://www.youtube.com/watch?v=tIeHLnjs5U8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example in PyTorch\n",
    "\n",
    "Let's take a look at a single training step.\n",
    "For this example, we load a pretrained resnet18 model from ``torchvision``.\n",
    "We create a random data tensor to represent a single image with 3 channels, and height & width of 64,\n",
    "and its corresponding ``label`` initialized to some random values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**download the model and save the model into path to be used later without downloading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "print(model)\n",
    "\n",
    "PATH = r'D:\\AI\\Lab\\resnetModel\\resnet18.pth'\n",
    "#save model parameters in dictionary\n",
    "torch.save(model.state_dict(), PATH) #state_dict is simply a Python dictionary object that maps each layer to its parameter tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load the model from path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=None)\n",
    "\n",
    "PATH = r'D:\\AI\\Lab\\resnetModel\\resnet18.pth'\n",
    "#LOAD model parameters in dictionary\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**change number of features if needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=5, bias=True)\n"
     ]
    }
   ],
   "source": [
    "n_features = 5\n",
    "model.fc = nn.Linear(model.fc.in_features, n_features)\n",
    "print(model.fc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create input data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n",
      "tensor([[[[0.1033, 0.4358, 0.3529,  ..., 0.1865, 0.7903, 0.3186],\n",
      "          [0.6606, 0.5478, 0.2796,  ..., 0.5896, 0.4598, 0.4833],\n",
      "          [0.1972, 0.9118, 0.3527,  ..., 0.3543, 0.0954, 0.3533],\n",
      "          ...,\n",
      "          [0.9667, 0.5492, 0.4358,  ..., 0.3313, 0.1753, 0.1909],\n",
      "          [0.4517, 0.2467, 0.0828,  ..., 0.8535, 0.5876, 0.0445],\n",
      "          [0.9913, 0.5371, 0.6573,  ..., 0.9180, 0.3586, 0.4642]],\n",
      "\n",
      "         [[0.1290, 0.1113, 0.2680,  ..., 0.8254, 0.3608, 0.7715],\n",
      "          [0.2252, 0.4883, 0.3717,  ..., 0.1680, 0.1650, 0.2221],\n",
      "          [0.0621, 0.9562, 0.0274,  ..., 0.1112, 0.5241, 0.8643],\n",
      "          ...,\n",
      "          [0.3892, 0.2721, 0.4071,  ..., 0.0379, 0.2711, 0.2863],\n",
      "          [0.9161, 0.4244, 0.7536,  ..., 0.6805, 0.3556, 0.8560],\n",
      "          [0.8922, 0.1264, 0.0643,  ..., 0.0283, 0.3656, 0.3289]],\n",
      "\n",
      "         [[0.0033, 0.4019, 0.2627,  ..., 0.8854, 0.4746, 0.5957],\n",
      "          [0.8635, 0.3647, 0.1779,  ..., 0.3657, 0.6348, 0.2939],\n",
      "          [0.0145, 0.3776, 0.6392,  ..., 0.1735, 0.6879, 0.7400],\n",
      "          ...,\n",
      "          [0.8979, 0.5578, 0.6294,  ..., 0.2375, 0.0237, 0.2302],\n",
      "          [0.0257, 0.1220, 0.3998,  ..., 0.9354, 0.1239, 0.7934],\n",
      "          [0.9067, 0.3213, 0.2693,  ..., 0.0888, 0.4386, 0.1726]]]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(1, 3, 64, 64)\n",
    "print(data.size())\n",
    "#x= torch.count_nonzero(data, dim=-2)\n",
    "#print(x)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create labels (random in this training); the size of the labels should be compatible with features (and therefore with predictions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2630, 0.8629, 0.9487, 0.0269, 0.6449]])\n",
      "torch.Size([1, 5])\n",
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "labels = torch.rand(1, 5)\n",
    "print(labels)\n",
    "print(labels.size())\n",
    "x= torch.count_nonzero(labels, dim=+1)\n",
    "#print(labels)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we run the input data through the model through each of its layers to make a prediction.\n",
    "This is the *forward pass*.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.1747, -5.8342, -5.2481, -4.0533, -3.9232]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "prediction = model(data) # forward pass\n",
    "print(prediction)\n",
    "#x= torch.count_nonzero(prediction, dim=+1)\n",
    "print(prediction.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We use the model's prediction and the corresponding label to calculate the error (*loss*).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-26.9800, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next step is to backpropagate this error through the network.\n",
    "Backward propagation is kicked off when we call ``.backward()`` on the error tensor.\n",
    "Autograd then calculates and stores the gradients for each model parameter in the parameter's ``.grad`` attribute.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we load an optimizer, in this case stochastic gradient descent **SGD** with a learning rate of 0.01 and momentum of 0.9.\n",
    "We register all the parameters of the model in the optimizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we call ``.step()`` to initiate gradient descent. The optimizer adjusts each parameter by its gradient stored in ``.grad``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation in Autograd\n",
    "\n",
    "**Let's take a look at how ``autograd`` collects gradients. We create two tensors ``a`` and ``b`` with\n",
    "``requires_grad=True``. This signals to ``autograd`` that every operation on them should be tracked.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True)\n",
      "tensor([6., 4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We create another tensor ``Q`` from ``a`` and ``b``.**\n",
    "\n",
    "\\begin{align}Q = 3a^3 - b^2\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's assume ``a`` and ``b`` to be parameters of an NN, and ``Q``\n",
    "to be the error. In NN training, we want gradients of the error\n",
    "w.r.t. parameters, i.e.**\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial a} = 9a^2\\end{align}\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial b} = -2b\\end{align}\n",
    "\n",
    "\n",
    "**When we call ``.backward()`` on ``Q``, autograd calculates these gradients\n",
    "and stores them in the respective tensors' ``.grad`` attribute.**\n",
    "\n",
    "**We need to explicitly pass a ``gradient`` argument in ``Q.backward()`` because it is a vector.\n",
    "``gradient`` is a tensor of the same shape as ``Q``, and it represents the\n",
    "gradient of Q w.r.t. itself, i.e.**\n",
    "\n",
    "\\begin{align}\\frac{dQ}{dQ} = 1\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients are now deposited in ``a.grad`` and ``b.grad``**\n",
    "\n",
    "**check if collected gradients are correct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph\n",
    "\n",
    "\n",
    "Conceptually, autograd keeps a record of data (tensors) & all executed\n",
    "operations (along with the resulting new tensors) in a directed acyclic\n",
    "graph (DAG) consisting of [Function] (https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
    "objects. In this DAG, leaves are the input tensors, roots are the output\n",
    "tensors. By tracing this graph from roots to leaves, you can\n",
    "automatically compute the gradients using the chain rule.\n",
    "\n",
    "In a forward pass, autograd does two things simultaneously:\n",
    "\n",
    "- run the requested operation to compute a resulting tensor, and\n",
    "- maintain the operationâ€™s *gradient function* in the DAG.\n",
    "\n",
    "The backward pass kicks off when ``.backward()`` is called on the DAG\n",
    "root. ``autograd`` then:\n",
    "\n",
    "- computes the gradients from each ``.grad_fn``,\n",
    "- accumulates them in the respective tensorâ€™s ``.grad`` attribute, and\n",
    "- using the chain rule, propagates all the way to the leaf tensors.**\n",
    "\n",
    "\n",
    "** NOTE: DAGs are dynamic in PyTorch**\n",
    "  An important thing to note is that the graph is recreated from scratch; after each\n",
    "  ``.backward()`` call, autograd starts populating a new graph. This is\n",
    "  exactly what allows you to use control flow statements in your model;\n",
    "  you can change the shape, size and operations at every iteration if\n",
    "  needed.</p></div>\n",
    "\n",
    "\n",
    "### Exclusion from the DAG\n",
    "\n",
    "\n",
    "**``torch.autograd`` tracks operations on all tensors which have their\n",
    "``requires_grad`` flag set to ``True``. For tensors that donâ€™t require\n",
    "gradients, setting this attribute to ``False`` excludes it from the\n",
    "gradient computation DAG.**\n",
    "\n",
    "**The output tensor of an operation will require gradients even if only a\n",
    "single input tensor has ``requires_grad=True``.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients? : False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In a NN, parameters that don't compute gradients are usually called **frozen parameters**.\n",
    "It is useful to \"freeze\" part of your model if you know in advance that you won't need the gradients of those parameters\n",
    "(this offers some performance benefits by reducing autograd computations).**\n",
    "\n",
    "**Another common usecase where exclusion from the DAG is important is for transfer learning** (see\n",
    "[finetuning a pretrained network](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html))\n",
    "\n",
    "**In transfer learning, we freeze most of the model and typically only modify the classifier layers (last fully connected layers) to make predictions on new labels.\n",
    "Let's walk through a small example to demonstrate this. As before, we load a pretrained resnet18 model, and freeze all the parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "[[[[-1.04193492e-02 -6.13560760e-03 -1.80977571e-03 ...  5.66148497e-02\n",
      "     1.70833264e-02 -1.26938839e-02]\n",
      "   [ 1.10827060e-02  9.52757336e-03 -1.09926917e-01 ... -2.71237552e-01\n",
      "    -1.29074901e-01  3.74236982e-03]\n",
      "   [-6.94335019e-03  5.90889715e-02  2.95482218e-01 ...  5.19718885e-01\n",
      "     2.56324232e-01  6.35725409e-02]\n",
      "   ...\n",
      "   [-2.75347717e-02  1.60450842e-02  7.25949779e-02 ... -3.32849443e-01\n",
      "    -4.20575589e-01 -2.57814825e-01]\n",
      "   [ 3.06125693e-02  4.09597456e-02  6.28496706e-02 ...  4.13839936e-01\n",
      "     3.93589228e-01  1.66058436e-01]\n",
      "   [-1.37361288e-02 -3.67455627e-03 -2.40843575e-02 ... -1.50699466e-01\n",
      "    -8.22297409e-02 -5.78277605e-03]]\n",
      "\n",
      "  [[-1.13972435e-02 -2.66190711e-02 -3.46408449e-02 ...  3.25206555e-02\n",
      "     6.62208884e-04 -2.57434826e-02]\n",
      "   [ 4.56866063e-02  3.36034410e-02 -1.04527377e-01 ... -3.12528163e-01\n",
      "    -1.60506710e-01 -1.28256797e-03]\n",
      "   [-8.37297121e-04  9.84199569e-02  4.02104735e-01 ...  7.07887888e-01\n",
      "     3.68871152e-01  1.24553785e-01]\n",
      "   ...\n",
      "   [-5.59257828e-02 -5.22390613e-03  2.70805229e-02 ... -4.61781323e-01\n",
      "    -5.70803106e-01 -3.65522057e-01]\n",
      "   [ 3.28604244e-02  5.55744171e-02  9.96695608e-02 ...  5.46364546e-01\n",
      "     4.82761741e-01  1.98674709e-01]\n",
      "   [ 5.30512584e-03  6.69381162e-03 -1.72543116e-02 ... -1.48223922e-01\n",
      "    -7.72484839e-02  7.21829361e-04]]\n",
      "\n",
      "  [[-2.03150185e-03 -9.16166510e-03  2.12094728e-02 ...  8.91766623e-02\n",
      "     3.36553939e-02 -2.01015621e-02]\n",
      "   [ 1.53981987e-02 -1.86480097e-02 -1.25907555e-01 ... -2.53420591e-01\n",
      "    -1.29799217e-01 -2.79753171e-02]\n",
      "   [ 9.84544959e-03  4.90468368e-02  2.16993466e-01 ...  3.48717779e-01\n",
      "     1.04332238e-01  1.84127670e-02]\n",
      "   ...\n",
      "   [-2.83560958e-02  1.84042398e-02  9.86470878e-02 ... -1.17399871e-01\n",
      "    -2.57603168e-01 -1.54506922e-01]\n",
      "   [ 2.07663588e-02 -2.62859208e-03 -3.78245153e-02 ...  2.41411939e-01\n",
      "     2.43448958e-01  1.17957450e-01]\n",
      "   [ 7.46839854e-04  7.76774716e-04 -1.00502959e-02 ... -1.48649633e-01\n",
      "    -1.17536455e-01 -3.83497998e-02]]]\n",
      "\n",
      "\n",
      " [[[-4.41544922e-03 -4.06447472e-03  3.15890298e-03 ... -3.70264687e-02\n",
      "    -2.51577571e-02 -4.79449034e-02]\n",
      "   [ 5.13104387e-02  5.34019992e-02  8.04356337e-02 ...  1.44799873e-01\n",
      "     1.42872006e-01  1.23124696e-01]\n",
      "   [-7.33367167e-03  2.17552902e-03  3.75804007e-02 ...  6.15173243e-02\n",
      "     8.03241059e-02  1.17152534e-01]\n",
      "   ...\n",
      "   [-2.67537534e-02 -1.22973405e-01 -1.36528090e-01 ... -1.40680373e-01\n",
      "    -1.11554153e-01 -4.95556854e-02]\n",
      "   [ 2.35237014e-02 -1.72883514e-02 -1.11224009e-02 ... -1.88255329e-02\n",
      "    -2.33195107e-02 -2.94735525e-02]\n",
      "   [ 2.86887418e-02  2.16587912e-02  4.78883348e-02 ...  2.54977681e-02\n",
      "     3.53457108e-02  1.12798279e-02]]\n",
      "\n",
      "  [[ 4.69187129e-04  1.21525778e-02  4.20348495e-02 ...  4.64027524e-02\n",
      "     4.04228829e-02 -1.44389598e-02]\n",
      "   [ 4.34633493e-02  6.87785372e-02  1.32675707e-01 ...  2.86058962e-01\n",
      "     2.69051641e-01  2.09349096e-01]\n",
      "   [-5.76209389e-02 -2.26417035e-02  3.05469763e-02 ...  1.37627378e-01\n",
      "     1.65376350e-01  1.79464340e-01]\n",
      "   ...\n",
      "   [-1.08164437e-01 -2.52273858e-01 -2.97422051e-01 ... -2.85028785e-01\n",
      "    -2.14933053e-01 -1.03198312e-01]\n",
      "   [ 4.07091565e-02 -3.27706002e-02 -6.34504855e-02 ... -9.23602581e-02\n",
      "    -6.98757172e-02 -4.98408936e-02]\n",
      "   [ 8.29421282e-02  8.75795111e-02  1.01114720e-01 ...  5.27144223e-02\n",
      "     6.09681867e-02  4.11979072e-02]]\n",
      "\n",
      "  [[-1.63907371e-02 -1.38701471e-02  5.28098177e-03 ...  4.36979905e-02\n",
      "     2.27074679e-02 -4.59831804e-02]\n",
      "   [ 3.32022607e-02  4.20135558e-02  9.34995189e-02 ...  2.61617213e-01\n",
      "     2.29698762e-01  1.66935205e-01]\n",
      "   [-4.59871292e-02 -1.63650718e-02  2.68111173e-02 ...  1.49513558e-01\n",
      "     1.32159531e-01  1.35789618e-01]\n",
      "   ...\n",
      "   [-7.21294358e-02 -1.89023703e-01 -2.33891174e-01 ... -1.90375045e-01\n",
      "    -1.56090751e-01 -7.59736374e-02]\n",
      "   [ 5.11608273e-02 -2.58150715e-02 -6.93574846e-02 ... -5.89988343e-02\n",
      "    -6.15504868e-02 -4.45553176e-02]\n",
      "   [ 1.11741364e-01  7.89792091e-02  6.58485889e-02 ...  3.16172168e-02\n",
      "     2.52212957e-02  7.42566772e-03]]]\n",
      "\n",
      "\n",
      " [[[-7.08255925e-08 -6.43061711e-08 -7.38059782e-08 ... -9.79999797e-08\n",
      "    -1.09046582e-07 -8.34209501e-08]\n",
      "   [-6.11253714e-09  2.06126383e-09 -8.09221667e-09 ... -4.98404695e-08\n",
      "    -4.38357937e-08 -3.05376502e-09]\n",
      "   [ 7.19528899e-08  7.56161072e-08  5.92821792e-08 ... -9.75090675e-09\n",
      "    -1.09507414e-09  4.24423625e-08]\n",
      "   ...\n",
      "   [ 9.58887085e-08  1.00390203e-07  7.98171769e-08 ... -1.74907768e-08\n",
      "    -4.76658819e-08 -1.32652547e-08]\n",
      "   [ 1.29039933e-07  1.47616902e-07  1.74766029e-07 ...  1.32325141e-07\n",
      "     1.06284297e-07  9.33156841e-08]\n",
      "   [ 1.25584677e-07  1.36442040e-07  1.84310352e-07 ...  2.13986411e-07\n",
      "     1.77096226e-07  1.71659892e-07]]\n",
      "\n",
      "  [[-1.26900616e-07 -9.61386348e-08 -1.03717625e-07 ... -1.18081722e-07\n",
      "    -1.33090793e-07 -1.08195458e-07]\n",
      "   [-5.74124250e-08 -2.50548187e-08 -3.01148120e-08 ... -7.29221199e-08\n",
      "    -6.70224978e-08 -2.25741150e-08]\n",
      "   [ 2.18129550e-08  4.86084524e-08  3.12216315e-08 ... -1.86939424e-08\n",
      "    -7.95906097e-09  3.97501196e-08]\n",
      "   ...\n",
      "   [ 5.60132492e-08  7.55256622e-08  4.44957315e-08 ... -4.41277557e-08\n",
      "    -5.99301160e-08 -1.82472188e-08]\n",
      "   [ 7.76138194e-08  9.83476696e-08  1.04548796e-07 ...  6.32717629e-08\n",
      "     4.17807478e-08  4.59011034e-08]\n",
      "   [ 5.98335106e-08  7.10060988e-08  9.04370125e-08 ...  1.16542104e-07\n",
      "     8.75504895e-08  9.88368924e-08]]\n",
      "\n",
      "  [[-4.38100969e-08  1.32701627e-08  7.82754750e-09 ... -5.88044857e-09\n",
      "    -2.62172293e-08 -1.56494835e-08]\n",
      "   [ 4.16999981e-08  1.07775193e-07  1.09462299e-07 ...  7.64030332e-08\n",
      "     7.14500743e-08  9.76147021e-08]\n",
      "   [ 1.04358712e-07  1.65855937e-07  1.59331947e-07 ...  1.35172414e-07\n",
      "     1.34871996e-07  1.64485272e-07]\n",
      "   ...\n",
      "   [ 9.87634934e-08  1.50724290e-07  1.25466215e-07 ...  6.83155363e-08\n",
      "     6.83818513e-08  1.13668818e-07]\n",
      "   [ 9.14345080e-08  1.35758540e-07  1.37933682e-07 ...  1.16783120e-07\n",
      "     1.17228772e-07  1.43941719e-07]\n",
      "   [ 6.21827638e-08  8.81841871e-08  1.04564080e-07 ...  1.39408755e-07\n",
      "     1.33325230e-07  1.58439391e-07]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-6.18959889e-02 -3.02062165e-02  1.92251690e-02 ...  4.36647572e-02\n",
      "    -2.21139006e-02 -4.22141068e-02]\n",
      "   [-3.80606875e-02  6.07736176e-03  4.57970276e-02 ...  9.60289463e-02\n",
      "     5.92543483e-02  2.99581569e-02]\n",
      "   [-2.96724383e-02  2.77664955e-03  2.04570945e-02 ...  5.98279424e-02\n",
      "     4.14222479e-02  2.31343303e-02]\n",
      "   ...\n",
      "   [ 1.19159855e-02  4.57008369e-02  4.48921397e-02 ...  4.74194251e-02\n",
      "     2.22736206e-02 -5.49928332e-03]\n",
      "   [-3.24675888e-02 -1.22095039e-02  2.20228154e-02 ...  5.80613390e-02\n",
      "    -7.50331115e-03 -5.97356446e-02]\n",
      "   [-4.33138832e-02 -2.81623844e-02 -5.91259729e-03 ...  8.84601623e-02\n",
      "     8.44061840e-03 -5.00187576e-02]]\n",
      "\n",
      "  [[-6.12921007e-02 -1.40039856e-02  1.72292739e-02 ...  1.83485411e-02\n",
      "    -3.27079184e-02 -4.10595760e-02]\n",
      "   [-3.15064266e-02  2.44596470e-02  4.55163121e-02 ...  6.68062270e-02\n",
      "     4.66866978e-02  3.32477652e-02]\n",
      "   [-3.22164223e-02  2.07181312e-02  2.33427119e-02 ...  3.52654569e-02\n",
      "     3.64779048e-02  3.12914699e-02]\n",
      "   ...\n",
      "   [ 1.77394412e-02  6.10403121e-02  4.82467413e-02 ...  3.77850793e-02\n",
      "     2.88937818e-02  1.39844017e-02]\n",
      "   [-1.08897937e-02  2.20786426e-02  4.27373610e-02 ...  6.02471195e-02\n",
      "     1.61973611e-02 -1.24926353e-02]\n",
      "   [-2.22842228e-02  1.32196667e-02  3.08968853e-02 ...  1.04033783e-01\n",
      "     4.01190370e-02 -5.33097656e-03]]\n",
      "\n",
      "  [[-8.53216648e-02 -4.26034331e-02  6.81449426e-03 ...  3.07509620e-02\n",
      "    -3.48184444e-02 -4.99447323e-02]\n",
      "   [-2.92147808e-02  1.81646701e-02  5.10917716e-02 ...  9.02002379e-02\n",
      "     5.34377545e-02  4.01689932e-02]\n",
      "   [-3.99319753e-02 -1.11000857e-03  9.61759035e-03 ...  2.41135377e-02\n",
      "     2.62980182e-02  2.54888162e-02]\n",
      "   ...\n",
      "   [-3.18897632e-03  3.04543786e-02  1.63163170e-02 ...  5.50542399e-03\n",
      "    -6.26886589e-03 -8.46376922e-03]\n",
      "   [-2.29949225e-02 -2.82112625e-03  2.32031066e-02 ...  3.58882472e-02\n",
      "    -1.42963966e-02 -3.24192196e-02]\n",
      "   [-9.88942105e-03  7.05419807e-03  1.06591685e-02 ...  7.04952031e-02\n",
      "     1.29961446e-02 -8.34174082e-03]]]\n",
      "\n",
      "\n",
      " [[[-7.86986947e-03  1.99110340e-02  3.42082307e-02 ...  2.86937729e-02\n",
      "     1.28200939e-02  1.81421936e-02]\n",
      "   [ 8.79424438e-03 -3.28754634e-02 -3.57130840e-02 ...  7.25334212e-02\n",
      "     4.58892770e-02  5.23831472e-02]\n",
      "   [-3.61219235e-02 -1.18783817e-01 -1.37665018e-01 ...  3.38111594e-02\n",
      "     3.78059186e-02  2.69435700e-02]\n",
      "   ...\n",
      "   [ 1.73219480e-02  3.95891396e-03 -8.22687801e-03 ...  2.75430735e-03\n",
      "     1.83131695e-02  1.60573144e-02]\n",
      "   [-9.50069749e-04  1.64283551e-02  1.71559546e-02 ...  3.36719025e-03\n",
      "     2.28573643e-02  6.57825207e-04]\n",
      "   [ 6.17269706e-03  2.71449108e-02  1.43402126e-02 ...  7.58674741e-03\n",
      "     1.87701210e-02  1.56242484e-02]]\n",
      "\n",
      "  [[-1.34233981e-02 -5.06963348e-04  8.09592940e-03 ... -6.09628158e-03\n",
      "     9.23413783e-03  1.57506131e-02]\n",
      "   [-1.83428340e-02 -6.79823384e-02 -7.06852525e-02 ...  2.98554841e-02\n",
      "     2.62637381e-02  2.37733498e-02]\n",
      "   [-5.43590896e-02 -1.46625280e-01 -1.62112728e-01 ...  1.17812213e-02\n",
      "     3.24773267e-02  1.19799618e-02]\n",
      "   ...\n",
      "   [ 8.36862251e-04 -1.75638851e-02 -1.95349175e-02 ... -4.13817493e-03\n",
      "     2.46576872e-02  1.28927017e-02]\n",
      "   [-6.31829374e-04  1.17881205e-02  2.48103458e-02 ...  6.11054245e-03\n",
      "     3.92103046e-02  9.66964010e-03]\n",
      "   [-7.18314108e-03  6.69179251e-03  5.27234608e-03 ... -7.60772778e-03\n",
      "     2.72529256e-02  1.77347101e-02]]\n",
      "\n",
      "  [[-2.37525732e-04 -4.93425271e-03  2.29912158e-03 ... -4.79575321e-02\n",
      "    -2.61544343e-02 -2.35251077e-02]\n",
      "   [-3.30529147e-04 -5.15019782e-02 -5.99774159e-02 ... -1.73687786e-02\n",
      "    -2.33372916e-02 -3.73122953e-02]\n",
      "   [-2.26742327e-02 -9.94124413e-02 -1.11760668e-01 ... -1.17246499e-02\n",
      "    -8.37444142e-03 -4.06151451e-02]\n",
      "   ...\n",
      "   [ 1.14366515e-02 -8.03133566e-03 -1.49554946e-03 ... -3.41327079e-02\n",
      "    -8.72666948e-03 -2.35259868e-02]\n",
      "   [ 2.95217149e-03  6.77699863e-04  1.99329928e-02 ... -2.20019575e-02\n",
      "     1.48137799e-02 -1.44867664e-02]\n",
      "   [-1.90853141e-02 -2.94298362e-02 -2.32837927e-02 ... -4.85872105e-02\n",
      "    -1.30492402e-02 -2.43681818e-02]]]\n",
      "\n",
      "\n",
      " [[[-3.62960026e-02  7.19964923e-03  1.91004705e-02 ...  1.96015965e-02\n",
      "     1.48704471e-02 -1.72981713e-02]\n",
      "   [-1.10605936e-02  8.56652185e-02  1.26670912e-01 ...  1.37441559e-02\n",
      "    -5.50358673e-05 -3.01621687e-02]\n",
      "   [ 1.13218442e-01  1.86340690e-01  5.06582409e-02 ... -1.73326597e-01\n",
      "    -7.20410421e-02 -6.24741353e-02]\n",
      "   ...\n",
      "   [-5.30620553e-02 -2.57809967e-01 -2.67472982e-01 ...  2.67807007e-01\n",
      "     1.43442497e-01  5.51452339e-02]\n",
      "   [-2.10086573e-02 -2.99689230e-02  1.02446362e-01 ...  2.08431229e-01\n",
      "    -4.15184209e-03 -3.81182134e-02]\n",
      "   [-2.21554693e-02  1.23802889e-02  8.43015537e-02 ... -4.49915268e-02\n",
      "    -1.46874920e-01 -9.08901840e-02]]\n",
      "\n",
      "  [[-5.39685274e-03  3.27991173e-02  1.54856332e-02 ... -7.74509693e-03\n",
      "     3.02292476e-03  1.12164102e-03]\n",
      "   [ 6.17226921e-02  1.48987576e-01  1.46449983e-01 ... -2.88972668e-02\n",
      "    -2.02270858e-02 -9.18778591e-03]\n",
      "   [ 1.61458209e-01  2.08861113e-01 -2.55890042e-02 ... -2.72777557e-01\n",
      "    -1.07354373e-01 -6.29712045e-02]\n",
      "   ...\n",
      "   [-1.37226492e-01 -4.08630282e-01 -3.85510594e-01 ...  4.08456713e-01\n",
      "     2.62018144e-01  1.34911031e-01]\n",
      "   [-5.93877137e-02 -6.11872971e-02  1.41966507e-01 ...  3.57799590e-01\n",
      "     9.08933952e-02 -1.73917355e-03]\n",
      "   [ 7.86126684e-03  5.84031455e-02  1.53385386e-01 ...  4.70448919e-02\n",
      "    -1.00952394e-01 -9.79195312e-02]]\n",
      "\n",
      "  [[-5.67989750e-03  1.34250745e-02 -2.64611989e-02 ...  4.48813755e-03\n",
      "     2.06658174e-03  1.39018288e-02]\n",
      "   [ 6.59430120e-03  4.51813675e-02  6.02603815e-02 ...  1.43675376e-02\n",
      "    -5.07247448e-03  4.05052630e-03]\n",
      "   [ 5.52570038e-02  1.23974383e-01  4.31931019e-02 ... -1.44855112e-01\n",
      "    -7.44885206e-02 -5.75329512e-02]\n",
      "   ...\n",
      "   [-3.15132923e-02 -1.63338572e-01 -1.57948330e-01 ...  2.29039595e-01\n",
      "     1.20172620e-01  7.19984472e-02]\n",
      "   [-1.04558002e-02 -1.12477632e-03  8.45818743e-02 ...  1.57484815e-01\n",
      "     2.21423395e-02 -1.00825056e-02]\n",
      "   [-4.86388523e-03 -5.00652753e-03  3.63407098e-02 ... -2.43614614e-02\n",
      "    -7.11945146e-02 -6.67875111e-02]]]]\n",
      "+++++++++++++++++++++++++++++++++++++\n",
      "Linear(in_features=512, out_features=1000, bias=True)\n",
      "[[-0.01847404 -0.07046092 -0.05177157 ... -0.03902964  0.17350928\n",
      "  -0.04097604]\n",
      " [-0.08179162 -0.09436987  0.01735497 ...  0.20284113 -0.02478185\n",
      "   0.03717173]\n",
      " [-0.03316446 -0.05656897 -0.02416519 ... -0.03440204 -0.0226594\n",
      "   0.01970495]\n",
      " ...\n",
      " [-0.01030026  0.00328042 -0.03586326 ... -0.02792347 -0.01145804\n",
      "   0.01275916]\n",
      " [-0.03587865 -0.03529599 -0.0296024  ... -0.03296117 -0.01102222\n",
      "  -0.05125643]\n",
      " [ 0.00212775 -0.02483878 -0.08292021 ...  0.04173126 -0.05003\n",
      "   0.06632734]]\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "#model = torchvision.models.resnet18(pretrained=True)\n",
    "model = resnet18(weights=None)\n",
    "\n",
    "PATH = r'D:\\AI\\Lab\\resnetModel\\resnet18.pth'\n",
    "#LOAD model parameters in dictionary\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#print(model)\n",
    "\n",
    "\n",
    "\n",
    "print(model.conv1 )\n",
    "print(model.conv1.weight.detach().numpy() )\n",
    "print(\"+++++++++++++++++++++++++++++++++++++\")\n",
    "print(model.fc )\n",
    "print(model.fc.weight.detach().numpy() )\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's say we want to finetune the model on a new dataset with 10 labels.\n",
    "In resnet, the classifier is the last linear layer ``model.fc``.\n",
    "We can simply replace it with a new linear layer (unfrozen by default)\n",
    "that acts as our classifier.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now all parameters in the model, except the parameters of ``model.fc``, are frozen.\n",
    "The only parameters that compute gradients are the weights and bias of ``model.fc``.\n",
    "This means also that during traininig, the only parameters that are computing gradients (and hence updated in gradient descent)\n",
    "are the weights and bias of the classifier (`model.fc`).**\n",
    "\n",
    "### NOTE:\n",
    " The same exclusionary functionality is available as a context manager in\n",
    "[`torch.no_grad()`] (https://pytorch.org/docs/stable/generated/torch.no_grad.html), that we will be using for *Transfer Learning*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "[[[[-1.04193492e-02 -6.13560760e-03 -1.80977571e-03 ...  5.66148497e-02\n",
      "     1.70833264e-02 -1.26938839e-02]\n",
      "   [ 1.10827060e-02  9.52757336e-03 -1.09926917e-01 ... -2.71237552e-01\n",
      "    -1.29074901e-01  3.74236982e-03]\n",
      "   [-6.94335019e-03  5.90889715e-02  2.95482218e-01 ...  5.19718885e-01\n",
      "     2.56324232e-01  6.35725409e-02]\n",
      "   ...\n",
      "   [-2.75347717e-02  1.60450842e-02  7.25949779e-02 ... -3.32849443e-01\n",
      "    -4.20575589e-01 -2.57814825e-01]\n",
      "   [ 3.06125693e-02  4.09597456e-02  6.28496706e-02 ...  4.13839936e-01\n",
      "     3.93589228e-01  1.66058436e-01]\n",
      "   [-1.37361288e-02 -3.67455627e-03 -2.40843575e-02 ... -1.50699466e-01\n",
      "    -8.22297409e-02 -5.78277605e-03]]\n",
      "\n",
      "  [[-1.13972435e-02 -2.66190711e-02 -3.46408449e-02 ...  3.25206555e-02\n",
      "     6.62208884e-04 -2.57434826e-02]\n",
      "   [ 4.56866063e-02  3.36034410e-02 -1.04527377e-01 ... -3.12528163e-01\n",
      "    -1.60506710e-01 -1.28256797e-03]\n",
      "   [-8.37297121e-04  9.84199569e-02  4.02104735e-01 ...  7.07887888e-01\n",
      "     3.68871152e-01  1.24553785e-01]\n",
      "   ...\n",
      "   [-5.59257828e-02 -5.22390613e-03  2.70805229e-02 ... -4.61781323e-01\n",
      "    -5.70803106e-01 -3.65522057e-01]\n",
      "   [ 3.28604244e-02  5.55744171e-02  9.96695608e-02 ...  5.46364546e-01\n",
      "     4.82761741e-01  1.98674709e-01]\n",
      "   [ 5.30512584e-03  6.69381162e-03 -1.72543116e-02 ... -1.48223922e-01\n",
      "    -7.72484839e-02  7.21829361e-04]]\n",
      "\n",
      "  [[-2.03150185e-03 -9.16166510e-03  2.12094728e-02 ...  8.91766623e-02\n",
      "     3.36553939e-02 -2.01015621e-02]\n",
      "   [ 1.53981987e-02 -1.86480097e-02 -1.25907555e-01 ... -2.53420591e-01\n",
      "    -1.29799217e-01 -2.79753171e-02]\n",
      "   [ 9.84544959e-03  4.90468368e-02  2.16993466e-01 ...  3.48717779e-01\n",
      "     1.04332238e-01  1.84127670e-02]\n",
      "   ...\n",
      "   [-2.83560958e-02  1.84042398e-02  9.86470878e-02 ... -1.17399871e-01\n",
      "    -2.57603168e-01 -1.54506922e-01]\n",
      "   [ 2.07663588e-02 -2.62859208e-03 -3.78245153e-02 ...  2.41411939e-01\n",
      "     2.43448958e-01  1.17957450e-01]\n",
      "   [ 7.46839854e-04  7.76774716e-04 -1.00502959e-02 ... -1.48649633e-01\n",
      "    -1.17536455e-01 -3.83497998e-02]]]\n",
      "\n",
      "\n",
      " [[[-4.41544922e-03 -4.06447472e-03  3.15890298e-03 ... -3.70264687e-02\n",
      "    -2.51577571e-02 -4.79449034e-02]\n",
      "   [ 5.13104387e-02  5.34019992e-02  8.04356337e-02 ...  1.44799873e-01\n",
      "     1.42872006e-01  1.23124696e-01]\n",
      "   [-7.33367167e-03  2.17552902e-03  3.75804007e-02 ...  6.15173243e-02\n",
      "     8.03241059e-02  1.17152534e-01]\n",
      "   ...\n",
      "   [-2.67537534e-02 -1.22973405e-01 -1.36528090e-01 ... -1.40680373e-01\n",
      "    -1.11554153e-01 -4.95556854e-02]\n",
      "   [ 2.35237014e-02 -1.72883514e-02 -1.11224009e-02 ... -1.88255329e-02\n",
      "    -2.33195107e-02 -2.94735525e-02]\n",
      "   [ 2.86887418e-02  2.16587912e-02  4.78883348e-02 ...  2.54977681e-02\n",
      "     3.53457108e-02  1.12798279e-02]]\n",
      "\n",
      "  [[ 4.69187129e-04  1.21525778e-02  4.20348495e-02 ...  4.64027524e-02\n",
      "     4.04228829e-02 -1.44389598e-02]\n",
      "   [ 4.34633493e-02  6.87785372e-02  1.32675707e-01 ...  2.86058962e-01\n",
      "     2.69051641e-01  2.09349096e-01]\n",
      "   [-5.76209389e-02 -2.26417035e-02  3.05469763e-02 ...  1.37627378e-01\n",
      "     1.65376350e-01  1.79464340e-01]\n",
      "   ...\n",
      "   [-1.08164437e-01 -2.52273858e-01 -2.97422051e-01 ... -2.85028785e-01\n",
      "    -2.14933053e-01 -1.03198312e-01]\n",
      "   [ 4.07091565e-02 -3.27706002e-02 -6.34504855e-02 ... -9.23602581e-02\n",
      "    -6.98757172e-02 -4.98408936e-02]\n",
      "   [ 8.29421282e-02  8.75795111e-02  1.01114720e-01 ...  5.27144223e-02\n",
      "     6.09681867e-02  4.11979072e-02]]\n",
      "\n",
      "  [[-1.63907371e-02 -1.38701471e-02  5.28098177e-03 ...  4.36979905e-02\n",
      "     2.27074679e-02 -4.59831804e-02]\n",
      "   [ 3.32022607e-02  4.20135558e-02  9.34995189e-02 ...  2.61617213e-01\n",
      "     2.29698762e-01  1.66935205e-01]\n",
      "   [-4.59871292e-02 -1.63650718e-02  2.68111173e-02 ...  1.49513558e-01\n",
      "     1.32159531e-01  1.35789618e-01]\n",
      "   ...\n",
      "   [-7.21294358e-02 -1.89023703e-01 -2.33891174e-01 ... -1.90375045e-01\n",
      "    -1.56090751e-01 -7.59736374e-02]\n",
      "   [ 5.11608273e-02 -2.58150715e-02 -6.93574846e-02 ... -5.89988343e-02\n",
      "    -6.15504868e-02 -4.45553176e-02]\n",
      "   [ 1.11741364e-01  7.89792091e-02  6.58485889e-02 ...  3.16172168e-02\n",
      "     2.52212957e-02  7.42566772e-03]]]\n",
      "\n",
      "\n",
      " [[[-7.08255925e-08 -6.43061711e-08 -7.38059782e-08 ... -9.79999797e-08\n",
      "    -1.09046582e-07 -8.34209501e-08]\n",
      "   [-6.11253714e-09  2.06126383e-09 -8.09221667e-09 ... -4.98404695e-08\n",
      "    -4.38357937e-08 -3.05376502e-09]\n",
      "   [ 7.19528899e-08  7.56161072e-08  5.92821792e-08 ... -9.75090675e-09\n",
      "    -1.09507414e-09  4.24423625e-08]\n",
      "   ...\n",
      "   [ 9.58887085e-08  1.00390203e-07  7.98171769e-08 ... -1.74907768e-08\n",
      "    -4.76658819e-08 -1.32652547e-08]\n",
      "   [ 1.29039933e-07  1.47616902e-07  1.74766029e-07 ...  1.32325141e-07\n",
      "     1.06284297e-07  9.33156841e-08]\n",
      "   [ 1.25584677e-07  1.36442040e-07  1.84310352e-07 ...  2.13986411e-07\n",
      "     1.77096226e-07  1.71659892e-07]]\n",
      "\n",
      "  [[-1.26900616e-07 -9.61386348e-08 -1.03717625e-07 ... -1.18081722e-07\n",
      "    -1.33090793e-07 -1.08195458e-07]\n",
      "   [-5.74124250e-08 -2.50548187e-08 -3.01148120e-08 ... -7.29221199e-08\n",
      "    -6.70224978e-08 -2.25741150e-08]\n",
      "   [ 2.18129550e-08  4.86084524e-08  3.12216315e-08 ... -1.86939424e-08\n",
      "    -7.95906097e-09  3.97501196e-08]\n",
      "   ...\n",
      "   [ 5.60132492e-08  7.55256622e-08  4.44957315e-08 ... -4.41277557e-08\n",
      "    -5.99301160e-08 -1.82472188e-08]\n",
      "   [ 7.76138194e-08  9.83476696e-08  1.04548796e-07 ...  6.32717629e-08\n",
      "     4.17807478e-08  4.59011034e-08]\n",
      "   [ 5.98335106e-08  7.10060988e-08  9.04370125e-08 ...  1.16542104e-07\n",
      "     8.75504895e-08  9.88368924e-08]]\n",
      "\n",
      "  [[-4.38100969e-08  1.32701627e-08  7.82754750e-09 ... -5.88044857e-09\n",
      "    -2.62172293e-08 -1.56494835e-08]\n",
      "   [ 4.16999981e-08  1.07775193e-07  1.09462299e-07 ...  7.64030332e-08\n",
      "     7.14500743e-08  9.76147021e-08]\n",
      "   [ 1.04358712e-07  1.65855937e-07  1.59331947e-07 ...  1.35172414e-07\n",
      "     1.34871996e-07  1.64485272e-07]\n",
      "   ...\n",
      "   [ 9.87634934e-08  1.50724290e-07  1.25466215e-07 ...  6.83155363e-08\n",
      "     6.83818513e-08  1.13668818e-07]\n",
      "   [ 9.14345080e-08  1.35758540e-07  1.37933682e-07 ...  1.16783120e-07\n",
      "     1.17228772e-07  1.43941719e-07]\n",
      "   [ 6.21827638e-08  8.81841871e-08  1.04564080e-07 ...  1.39408755e-07\n",
      "     1.33325230e-07  1.58439391e-07]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-6.18959889e-02 -3.02062165e-02  1.92251690e-02 ...  4.36647572e-02\n",
      "    -2.21139006e-02 -4.22141068e-02]\n",
      "   [-3.80606875e-02  6.07736176e-03  4.57970276e-02 ...  9.60289463e-02\n",
      "     5.92543483e-02  2.99581569e-02]\n",
      "   [-2.96724383e-02  2.77664955e-03  2.04570945e-02 ...  5.98279424e-02\n",
      "     4.14222479e-02  2.31343303e-02]\n",
      "   ...\n",
      "   [ 1.19159855e-02  4.57008369e-02  4.48921397e-02 ...  4.74194251e-02\n",
      "     2.22736206e-02 -5.49928332e-03]\n",
      "   [-3.24675888e-02 -1.22095039e-02  2.20228154e-02 ...  5.80613390e-02\n",
      "    -7.50331115e-03 -5.97356446e-02]\n",
      "   [-4.33138832e-02 -2.81623844e-02 -5.91259729e-03 ...  8.84601623e-02\n",
      "     8.44061840e-03 -5.00187576e-02]]\n",
      "\n",
      "  [[-6.12921007e-02 -1.40039856e-02  1.72292739e-02 ...  1.83485411e-02\n",
      "    -3.27079184e-02 -4.10595760e-02]\n",
      "   [-3.15064266e-02  2.44596470e-02  4.55163121e-02 ...  6.68062270e-02\n",
      "     4.66866978e-02  3.32477652e-02]\n",
      "   [-3.22164223e-02  2.07181312e-02  2.33427119e-02 ...  3.52654569e-02\n",
      "     3.64779048e-02  3.12914699e-02]\n",
      "   ...\n",
      "   [ 1.77394412e-02  6.10403121e-02  4.82467413e-02 ...  3.77850793e-02\n",
      "     2.88937818e-02  1.39844017e-02]\n",
      "   [-1.08897937e-02  2.20786426e-02  4.27373610e-02 ...  6.02471195e-02\n",
      "     1.61973611e-02 -1.24926353e-02]\n",
      "   [-2.22842228e-02  1.32196667e-02  3.08968853e-02 ...  1.04033783e-01\n",
      "     4.01190370e-02 -5.33097656e-03]]\n",
      "\n",
      "  [[-8.53216648e-02 -4.26034331e-02  6.81449426e-03 ...  3.07509620e-02\n",
      "    -3.48184444e-02 -4.99447323e-02]\n",
      "   [-2.92147808e-02  1.81646701e-02  5.10917716e-02 ...  9.02002379e-02\n",
      "     5.34377545e-02  4.01689932e-02]\n",
      "   [-3.99319753e-02 -1.11000857e-03  9.61759035e-03 ...  2.41135377e-02\n",
      "     2.62980182e-02  2.54888162e-02]\n",
      "   ...\n",
      "   [-3.18897632e-03  3.04543786e-02  1.63163170e-02 ...  5.50542399e-03\n",
      "    -6.26886589e-03 -8.46376922e-03]\n",
      "   [-2.29949225e-02 -2.82112625e-03  2.32031066e-02 ...  3.58882472e-02\n",
      "    -1.42963966e-02 -3.24192196e-02]\n",
      "   [-9.88942105e-03  7.05419807e-03  1.06591685e-02 ...  7.04952031e-02\n",
      "     1.29961446e-02 -8.34174082e-03]]]\n",
      "\n",
      "\n",
      " [[[-7.86986947e-03  1.99110340e-02  3.42082307e-02 ...  2.86937729e-02\n",
      "     1.28200939e-02  1.81421936e-02]\n",
      "   [ 8.79424438e-03 -3.28754634e-02 -3.57130840e-02 ...  7.25334212e-02\n",
      "     4.58892770e-02  5.23831472e-02]\n",
      "   [-3.61219235e-02 -1.18783817e-01 -1.37665018e-01 ...  3.38111594e-02\n",
      "     3.78059186e-02  2.69435700e-02]\n",
      "   ...\n",
      "   [ 1.73219480e-02  3.95891396e-03 -8.22687801e-03 ...  2.75430735e-03\n",
      "     1.83131695e-02  1.60573144e-02]\n",
      "   [-9.50069749e-04  1.64283551e-02  1.71559546e-02 ...  3.36719025e-03\n",
      "     2.28573643e-02  6.57825207e-04]\n",
      "   [ 6.17269706e-03  2.71449108e-02  1.43402126e-02 ...  7.58674741e-03\n",
      "     1.87701210e-02  1.56242484e-02]]\n",
      "\n",
      "  [[-1.34233981e-02 -5.06963348e-04  8.09592940e-03 ... -6.09628158e-03\n",
      "     9.23413783e-03  1.57506131e-02]\n",
      "   [-1.83428340e-02 -6.79823384e-02 -7.06852525e-02 ...  2.98554841e-02\n",
      "     2.62637381e-02  2.37733498e-02]\n",
      "   [-5.43590896e-02 -1.46625280e-01 -1.62112728e-01 ...  1.17812213e-02\n",
      "     3.24773267e-02  1.19799618e-02]\n",
      "   ...\n",
      "   [ 8.36862251e-04 -1.75638851e-02 -1.95349175e-02 ... -4.13817493e-03\n",
      "     2.46576872e-02  1.28927017e-02]\n",
      "   [-6.31829374e-04  1.17881205e-02  2.48103458e-02 ...  6.11054245e-03\n",
      "     3.92103046e-02  9.66964010e-03]\n",
      "   [-7.18314108e-03  6.69179251e-03  5.27234608e-03 ... -7.60772778e-03\n",
      "     2.72529256e-02  1.77347101e-02]]\n",
      "\n",
      "  [[-2.37525732e-04 -4.93425271e-03  2.29912158e-03 ... -4.79575321e-02\n",
      "    -2.61544343e-02 -2.35251077e-02]\n",
      "   [-3.30529147e-04 -5.15019782e-02 -5.99774159e-02 ... -1.73687786e-02\n",
      "    -2.33372916e-02 -3.73122953e-02]\n",
      "   [-2.26742327e-02 -9.94124413e-02 -1.11760668e-01 ... -1.17246499e-02\n",
      "    -8.37444142e-03 -4.06151451e-02]\n",
      "   ...\n",
      "   [ 1.14366515e-02 -8.03133566e-03 -1.49554946e-03 ... -3.41327079e-02\n",
      "    -8.72666948e-03 -2.35259868e-02]\n",
      "   [ 2.95217149e-03  6.77699863e-04  1.99329928e-02 ... -2.20019575e-02\n",
      "     1.48137799e-02 -1.44867664e-02]\n",
      "   [-1.90853141e-02 -2.94298362e-02 -2.32837927e-02 ... -4.85872105e-02\n",
      "    -1.30492402e-02 -2.43681818e-02]]]\n",
      "\n",
      "\n",
      " [[[-3.62960026e-02  7.19964923e-03  1.91004705e-02 ...  1.96015965e-02\n",
      "     1.48704471e-02 -1.72981713e-02]\n",
      "   [-1.10605936e-02  8.56652185e-02  1.26670912e-01 ...  1.37441559e-02\n",
      "    -5.50358673e-05 -3.01621687e-02]\n",
      "   [ 1.13218442e-01  1.86340690e-01  5.06582409e-02 ... -1.73326597e-01\n",
      "    -7.20410421e-02 -6.24741353e-02]\n",
      "   ...\n",
      "   [-5.30620553e-02 -2.57809967e-01 -2.67472982e-01 ...  2.67807007e-01\n",
      "     1.43442497e-01  5.51452339e-02]\n",
      "   [-2.10086573e-02 -2.99689230e-02  1.02446362e-01 ...  2.08431229e-01\n",
      "    -4.15184209e-03 -3.81182134e-02]\n",
      "   [-2.21554693e-02  1.23802889e-02  8.43015537e-02 ... -4.49915268e-02\n",
      "    -1.46874920e-01 -9.08901840e-02]]\n",
      "\n",
      "  [[-5.39685274e-03  3.27991173e-02  1.54856332e-02 ... -7.74509693e-03\n",
      "     3.02292476e-03  1.12164102e-03]\n",
      "   [ 6.17226921e-02  1.48987576e-01  1.46449983e-01 ... -2.88972668e-02\n",
      "    -2.02270858e-02 -9.18778591e-03]\n",
      "   [ 1.61458209e-01  2.08861113e-01 -2.55890042e-02 ... -2.72777557e-01\n",
      "    -1.07354373e-01 -6.29712045e-02]\n",
      "   ...\n",
      "   [-1.37226492e-01 -4.08630282e-01 -3.85510594e-01 ...  4.08456713e-01\n",
      "     2.62018144e-01  1.34911031e-01]\n",
      "   [-5.93877137e-02 -6.11872971e-02  1.41966507e-01 ...  3.57799590e-01\n",
      "     9.08933952e-02 -1.73917355e-03]\n",
      "   [ 7.86126684e-03  5.84031455e-02  1.53385386e-01 ...  4.70448919e-02\n",
      "    -1.00952394e-01 -9.79195312e-02]]\n",
      "\n",
      "  [[-5.67989750e-03  1.34250745e-02 -2.64611989e-02 ...  4.48813755e-03\n",
      "     2.06658174e-03  1.39018288e-02]\n",
      "   [ 6.59430120e-03  4.51813675e-02  6.02603815e-02 ...  1.43675376e-02\n",
      "    -5.07247448e-03  4.05052630e-03]\n",
      "   [ 5.52570038e-02  1.23974383e-01  4.31931019e-02 ... -1.44855112e-01\n",
      "    -7.44885206e-02 -5.75329512e-02]\n",
      "   ...\n",
      "   [-3.15132923e-02 -1.63338572e-01 -1.57948330e-01 ...  2.29039595e-01\n",
      "     1.20172620e-01  7.19984472e-02]\n",
      "   [-1.04558002e-02 -1.12477632e-03  8.45818743e-02 ...  1.57484815e-01\n",
      "     2.21423395e-02 -1.00825056e-02]\n",
      "   [-4.86388523e-03 -5.00652753e-03  3.63407098e-02 ... -2.43614614e-02\n",
      "    -7.11945146e-02 -6.67875111e-02]]]]\n",
      "+++++++++++++++++++++++++++++++++++++\n",
      "Linear(in_features=512, out_features=5, bias=True)\n",
      "[[-0.07112074 -0.0572089  -0.01770304 ... -0.0743639  -0.05386572\n",
      "   0.00523281]\n",
      " [-0.03519568 -0.04920705 -0.04734587 ... -0.0236063  -0.04618131\n",
      "   0.00647987]\n",
      " [-0.04078649 -0.03792282 -0.04962552 ... -0.01565783 -0.02797341\n",
      "  -0.04186249]\n",
      " [-0.02313473 -0.05611637  0.00525921 ... -0.05718536 -0.0684391\n",
      "  -0.07255898]\n",
      " [-0.02438498 -0.06100997 -0.01687085 ...  0.00916051 -0.01980846\n",
      "  -0.05462944]]\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1 )\n",
    "print(model.conv1.weight.detach().numpy() )\n",
    "print(\"+++++++++++++++++++++++++++++++++++++\")\n",
    "print(model.fc )\n",
    "print(model.fc.weight.detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
